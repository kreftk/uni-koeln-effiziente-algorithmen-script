
Vor der Einführung machte Prof. Jünger eine ganz kurze informelle 
Vorstellung der kombinatorischen Optimierung, bei der er uns bat nicht 
mit zuschreiben. Dieser Teil ist deshalb hier nicht wieder gegeben. 

\section{Matrizen Exkurs}

Auf Bitten von Kommilitonen machte Prof. Jünger einen kleinen Exkurs in die
Matrizen--Schreibweise, die er verwendet. 

$A x = b\;$ $A \in \RR^{m\times n}\;$ $b \in \RR^{m}$\\
$A = (a_{i j}) = \left(\begin{array}{ccc}a_{11}&\ldots&a_{1n}\\
\vdots&&\vdots\\a_{m1}&\ldots&a_{m n}\end{array}\right)\;$
 $b=\left(\begin{array}{c}b_{1}\\\vdots\\b_{m}\end{array}\right)$

Matrizen-Multiplikation:\\
$A \in \RR^{m\times k}\;$ $B \in \RR^{k \times n};$ $C \in \RR^{m\times n}$ 

$A \cdot B = C$ läuft dann folgendermaßen:

\begin{equation*} 
c_{i j}= \sum^{k}_{p=1} a_{i p} * b_{pi} \; \; \forall\; {i} \;\;\; \mbox{für alle i
Zeilen}
\end{equation*}

Gleichungssysteme:\\
$A x=b$ dabei ist $x=\left(\begin{array}{c}x_{1}\\x_{2}\\\vdots\\x_{n}\end{array}\right) \in \RR^{n}$
anders geschrieben:
\[
\left(\begin{array}{c}a_{11}\\\vdots\\a_{m1}\end{array}\right)
x_{1}+
\left(\begin{array}{c}a_{12}\\\vdots\\a_{m2}\end{array}\right)
x_{2}+\ldots+
\left(\begin{array}{c}a_{1n}\\\vdots\\a_{m n}\end{array}\right)
x_{n} =
\left(\begin{array}{c}b_{1}\\\vdots\\b_{m}\end{array}\right)
\]

Form mit transponierten Vektoren:\\
$c \in \RR^{n}, \;\; y \in \RR^{m}$
\[\begin{array}{rcl} y^{T}A\ &=& c^{T}\\
y_{1}(a_{11}, \ldots, a_{1n}) + y_{2} (a_{21}, \ldots, a_{2n}), + \ldots +
 y_{m} (a_{m1}, \ldots, a_{m n}) &=& (c_{1}, \ldots c_{n})
\end{array}\]

Multiplikation von Vektoren:\\
$a,\; b \in \RR^{n}$\\

\[ab = \sum^{n}_{i=1} a_{i} b_{i} = (a_{1}, \ldots, a_{n})
\left(\begin{array}{c}b_{1}\\ \vdots\\ b_{n} \end{array}\right)\]
Eigentlich müsste man $a^{T}b$ schreiben, da $a$ ja ein transponierter
Vektor ist. Mathematiker vergessen so etwas aber schon mal gerne, das es ja
ohnehin klar ist... 


\section{Allgemeine Formulierungen von Optimierungsproblemen}

\subsection{Generisches Optimierungsproblem}
\begin{tabular}{ll}
Gegeben: & $ A \in \RR^{p \times r},\;  B \in \RR^{p\times r},\; 
C \in \RR^{q\times r}, \; D \in \RR^{q\times s} $\\
& $a \in \RR^{r}, \; b \in \RR^{s}, \; c \in \RR^{p},\; d \in \RR^{q}$\\
(LP)&$\max a^{T}x + b^{T}y$\\
&$A x+ B y = c$\\
&$C x+D y \leq d$\\
&$x \geq 0$\\
\end{tabular}\\

Dies ist die allgemeine Formulierung eines (linearen) generischen
Optimierungsproblems. Für ein ganzzahliges Optimierungsproblem (IP) gilt
zusätzlich zum LP noch:

\[x \in \ZZ^{r}, \;y \in \ZZ^{s} \]

Ein gemischt ganzzahliges Optimierungsproblem liegt vor wenn nur Teile
der Vektoren x und y integral (ganzzahlig) sind.


\subsection{Kombinatorisches Optimierungsproblem}

\begin{tabular}{ll}
Gegeben:& $E$ eine endliche Grundmenge\\
&$y \subseteqq 2^{E},\;$ y ist eine zulässige Teilmenge von $E$\\
&$c$: $E \rightarrow \RR$ (z.b. Zuweisung von Entfernungen zu den
Kanten)\\
&Für $F \subseteqq E$: $c(F) = \displaystyle \sum_{e\in F} c(e)$\\
Gesucht:& $I^{\star} \in  y$ mit $c(I^{\star}) \geqq c(I),\;$ für alle $I \in
 y$\\
\hspace{3mm} oder:&$I^{\star} \in  y$ mit $c(I^{\star}) \leqq c(I),\;$ für
alle $I \in  y$
\end{tabular}


\subsubsection{Kleine Aufwärmaufgabe}

Für $A \in \RR^{m\times n}, \; b \in \RR^{m}$ gilt:\\
($\exists x \in \RR^{n}$) $\;A x = b$ (Gleichungssystem ist lösbar)\\
genau dann wenn:\\
($\nexists y \in \RR^{m}$) $\;y^{T}A=0$, $\;y^{T} b=-1$

Hiernach erklärte Prof. Jünger noch kurz und informell die Triangulation
für Input-/Outputmodelle. In der Nächsten Stunde wurden zunächst 
Reihenfolgeprobleme mit einigen Beispielen im Graphenzeichnen und in der 
Flugplanung veranschaulicht.

\section{Einführung in die lineare Optimierung}

Es folgt ein Beispiel aus der Ölraffinierung: Beim Prozess des so genannten
"`Crackens"' wird Rohöl in drei Produkte aufgespalten: Schweres Öl (S),
mittelschweres Öl (M) und leichtes Öl (L). Nun gibt es zwei verschiedene
Crack-Prozesse die zu einer unterschiedlichen Aufteilung der Endprodukte
führen.

\[\mbox{Prozess 1: 10 ME (= Mengeneinheiten) Rohöl ergibt} \longrightarrow 
\left. \begin{array}{l}
\mbox{2 ME S}\\
\mbox{2 ME M}\\
\mbox{1 ME L}\\
\end{array}\right\} \mbox{Kosten 3 GE}\]

\[\mbox{Prozess 1: 10 ME (= Mengeneinheiten) Rohöl ergibt} \longrightarrow
\left.
\begin{array}{l}
\mbox{1 ME S}\\
\mbox{2 ME M}\\
\mbox{4 ME L}\\
\end{array}\right\}\mbox{Kosten 5 GE}\]

Die Firma hat Lieferverpflichtungen, deswegen muss sie mindestens 3 ME S, 5
ME M und 4 ME L produzieren.

\[\begin{array}{ll}
\mbox{Variablen:}& x_{1}\mbox{, Produktionsniveau von Crackprozess 1}\\
&x_{2}\mbox{, Produktionsniveau von Crackprozess 2}\\
&x_{1}\geqq 0, x_{2}\geqq 0\\
&\mbox{z.B. }x_{1} = 1,5 \L\Leftrightarrow \mbox{ 15 ME Rohöl für Prozess 1}
\end{array}\]

Damit erhalten wir das folgende lineare Optimierungsproblem:\\
\[\begin{array}{l}
\min\; 3x_{1}+5x_{2} \;\;\; \mbox{(Zielfunktion)}\\
\left.\begin{array}{l}\mbox{s.t.}\begin{array}[t]{rl}
(1)& 2x_{1} + x_{2} \geqq 3\\
(2)&2x_{1}+ 2x_{2} \geqq 5\\
(3)&x_{1}+ 4x_{2} \geqq 4\\
(4)&x_{1} \geqq 0\\
(5)&x_{2} \geqq 0\\
\end{array}\end{array}\right\}\mbox{Restriktionen}\end{array}
\]

Jeder Vektor {\scriptsize$\left(\begin{array}{c}x_{1}\\x_{2}\end{array}\right)$} $\in \RR^{2}$, der
(1), (2)...., (5) erfüllt heißt zulässige Lösung.

Graphische Darstellung (gerade keine Lust auf xfig, das bekommt ihr ja wohl
selber hin :)

Aus der n.v. Grafik kann man ganz einfach die optimale Lösung erkennen:
$x^{*} = \mbox{\scriptsize
$\left(\begin{array}{c}2\\0,5\end{array}\right)$}, Wert = 8,5$.

Nähme man nun aber eine beliebige Zielfunktion $a x_{1}+b x_{2}$ sowie $a < 0$ 
und definiere eine Folge von Punkten $x^{(n)}= \mbox{\scriptsize$\left(
\begin{array}{c}n\\ 0\end{array}\right)$}$ mit $n \geq 4$. Weiterhin gilt: 
$a x_{1}^{(n)} + b x_{2}^{(n)} =  n a$. Da a negativ 
ist ist der Wert der Zielfunktion offensichtlich nicht nach unten
beschränkt.

\subsection{Lösungen für Lineare Probleme}

\subsubsection{Simpler Lösungsalgorithmus für ein LP}  

Zunächst stellen wir eine Vermutung an. Wenn eine Optimallösung
existiert, so existiert eine die im Durchschnitt von n Hyperebenen liegt
(wird später bewiesen). Eine Hyperebene ist ein (n-1)-dimensionaler
Unterraum also im $R^{2}$ eine Gerade im $R^{3}$ eine Ebene, u.s.w.

Davon ausgehend konstruieren wie eine einfache Lösungsmethode. Man
schreibe (1), (2), ..., (5) als Gleichungen und löse alle ${5\choose 2}
= 10$ Gleichungssysteme mit 2 Gleichungen. Von allen Lösungen, die
zulässig sind, ist die mit dem kleinsten Zielfunktionswert optimal.

{\bf Aufwand:} Benötigen ${m\choose n}$ Aufrufe des Gleichungssytemlösers mit
n Variablen und n Gleichungen. $\rightarrow$ Viel zu langsam in der
Praxis.

\subsubsection{Idee des Simplexalgorithmus}

Der Simplexalgorithmus geht folgendermaßen vor:\\
(a) Finde eine erste Lösung wie im simplen Algorithmus.\\
(b) Gehe durch Austausch je einer Hyperebene zu einer benachbarten
Lösung deren Zielfunktionswertes nicht schlechter ist.
(c) Wiederhole (b) bis Optimalität festgestellt ist. ($\rightarrow$ via
Dualitätstheorie, die später noch behandelt wird).

\subsubsection{Bestimmung von Schranken}

Wir zeigen nun, dass $x^{*} =
\mbox{\scriptsize$\left(\begin{array}{c}2\\0,5\end{array}\right)$}$ optimal
ist:\\
{\bf Beobachtung 1:} Die Lösungsmenge einer Ungleichung ist invariant
unter Skalierung mit positiven Skalaren, z.B. (1):
\[\{x|2x_{1}+x_{2} \geqq 3\} = \{x|4x_{1}+2x_{2} \geqq 6\}\]

Negative Faktoren kehren die Ungleichung um.

{\bf Beobachtung 2:} Erfüllt ein Vektor zwei gleich gerichtete (beide
"`$\leqq$"' oder beide "`$\geqq$"') so auch die Summe der beiden
Ungleichungen. z.B. (1) + (3):
\[\begin{array}{llcl}
(1)&2x_{1}+x_{2} &\geqq& 3\\
(3)&x_{1}+4x_{2}&\geqq&4\\
\hline
&\underbrace{3x_{1} + 5x_{2}}_{ZF}&\geqq& 7
\end{array}\] 

Nun können wir behaupten, dass wir mindestens 7 GE ausgeben müssen.
$\rightarrow$ Jeder Vektor der (1)...(5) erfüllt hat einen ZF-Wert von
wenigstens 7. Also ist 7 eine untere Schranke für das Minimum. Nun versuchen
wir eine bessere Schranke zu finden:
\[\begin{array}{llcl}
(1)&2x_{1}+x_{2} &\geqq& 3\\
(2)&2x_{1}+2x_{2}&\geqq&5\\
\hline
&4x_{1} + 3x_{2}&\geqq& 8
\end{array}\]
In dieser Ungleichung ist der Koeffizient für $x_{1} = 4$  und ist damit
größer als der Koeffizient in der Zielfunktion. Damit ist dies Ungleichung
nicht zum Abschätzen einer unteren Schranke geeignet. Also ein erneuter
Versuch indem wir (2) mit 1,5 multiplizieren. Damit erhalten wir:
\[3x_{1}+3x_{2} \geqq 7,5\]

\subsubsection{Dualität}

Zwar haben wir nun eine bessere untere Schranke aber dieser Probier-Ansatz ist
etwas unbefriedigend. Darum nun ein systematischer Ansatz
zum Finden der Faktoren für die Gleichungen um eine Schranke zu finden. Da
die Nichtnegativ-Bedingungen in diesem Fall irrelevant sind suchen wir für
unser Problem drei Multiplikatoren ($y_{1}$, $y_{2}$, $y_{3}$) für die
Gleichungen (1) (2) (3).
\[\begin{array}{l}\max \; 3y_{1} + 5y_{2} + 4y_{3} \rightarrow \mbox{Die Rechte
Seite (Schranke) maximieren)}\\
\left.\begin{array}{l}\mbox{s.t.}\begin{array}[t]{lcl}
2y_{1}+2y_{2}+y_{3} &\leqq & 3 \\
y_{1}+2y_{2}+4y_{3} &\leqq & 5
\end{array} \end{array} \right\}\mbox{Damit die ZF-Koeffizienten nicht
überschritten werden}\end{array}\]
Damit haben wir nun das entsprechende duale Optimierungsproblem. Nach
dessen Konstruktion liefert der Zielfunktionswert jeder zulässigen Lösung
des dualen Problems eine untere Schranke für den Zielfunktionswert des
primalen Problems. Wir betrachten:
\[y^{*}=\left(\begin{array}{c}y^{*}_{1}\\y^{*}_{2}\\y^{*}_{3}\end{array}\right)
=\left(\begin{array}{c}0\\\frac{7}{6}\\\frac{2}{3}\end{array}\right)\] 
dies entspricht:
\[\begin{array}{llcl}
\frac{7}{6} \cdot (2):\; & \frac{14}{6}x_{1} + \frac{14}{6}x_{2} &\geqq&
\frac{35}{6}\\
\frac{2}{3} \cdot (3):\; & \frac{4}{6}x_{1} + \frac{16}{6}x_{2} &\geqq&
\frac{16}{6}\\
\hline
&3x_{1} + 5x_{2} &\geqq& 8,5
\end{array}
\]

\paragraph{Schwacher Dualitätssatz}
Es seien $c \in \RR^{n}$, $b \in \RR^{m}$, $A \in \RR^{m \times n}$ für die
Optimierungsaufgaben:

\begin{tabular}{ll}
(P)&$\min \{c^{T}x | A x \geqq b, \; x\geqq 0\}$\\
&d.h. bestimme $x^{*} \in \RR$ mit $A x^{*} \geqq b$, $x^{*} \geqq 0 $ und
$c^{T}x^{*}$ so klein wie möglich.\\
(D)&$\max \{y^{T}b|y^{T}A \leqq c^{T}, \; y \geqq 0\}$\\
&d.h. bestimme  $y^{*} \in \RR^{m}$ mit $g^{*T}A \leqq c^{T}, \; y ^{*}
\geqq 0$ und $y^{*T}b$ so groß wie möglich.
\end{tabular}

so gilt folgendes: Seien $x_{0} \in \RR^{n}$, $ y_{0} \in \RR^{m}$ Punkte
mit $A x_{0} \geqq b $, $x_{0} \geqq 0$ und $y_{0}^{T}A \leqq c^{T}$,
$y_{0} \geqq 0$ dann gilt $y_{0}^{T}b \leqq c^{T} x_{0}$\\
Beweis:
\[y_{0}^{T}b \leqq y_{0}^{T}(A x_{0}) = (y_{0}^{T}A)x_{0} \leqq c^{T}x_{0}
\; \; \; \; \mbox{q.e.d.}\] 

Der starke Dualitätssatz (später) besagt $y^{*T}b =c^{T}x^{*}$ für
Optimallösung $x^{*}$ von (P) und $y^{*}$ von (D).

\subsubsection{Ökonomische Interpretation des Dualen Programms}

Ein Manager der Ölraffinerie würde sich nun die Frage stellen, ob es sinnvoll
L, M uns S selbst herzustellen, oder ob man es auf dem Markt kaufen sollte 
b.z.w.  bei welchen Marktpreis eine Produktion nicht mehr lohnt. In diesem Fall
drückt $y_{1}$ den Preis in GE für S aus, $y_{2}$ für M, $y_{3}$ für L. Der
Marktpreis für die benötigten Mengen beträgt somit:
\[3y_{1} + 5y_{5} + 4y_{3} \; \; \mbox{GE}\]
der Ausstoß von Crackprozess 1 hat den Marktpreis:
\[2y_{1}+2y_{2}+3y_{3} \; \; \mbox{GE} > 3 \;\;\;\; \rightarrow \mbox{Produktion Lohnt}\]
der Ausstoß von Crackprozess 2 hat den Marktpreis:
\[y_{1}+2y_{2}+4y_{3} \;\; \mbox{GE} > 5 \;\;\;\; \rightarrow \mbox{Produktion Lohnt}\]
$y_{i}$ werden auch als Schattenpreise bezeichnet. In unserem Beispiel
betragen sie:
\[\begin{array}{lll}(S):\; y_{1}^{*}=0& (M):\; y_{2}^{*} =
\frac{7}{6}&(L):\;
y_{3}^{*} = \frac{2}{3}
\end{array}\]
Hier ist zu erkennen dass S praktisch "`wertlos"' für die Firma ist. In der
tat ist es in der Optimallösung im Überfluss vorhanden (also mehr als die
Lieferverpflichtungen der Firma verlangen. D.h. man würde das Öl zunächst
zu jedem Preis verkaufen (solange man die insgesamte Produktionsmenge
dadurch nicht erhöhen muss).

\subsection{Formen von Linearen Problemen \label{FormenLP}}

\subsubsection{Kanonische Form eines LPs}

Die Kanonische Form eines LPs lautet:
\[\begin{array}{lcl}
\max\; c^{T}x\\
A x&\leqq& b\\
x&\geqq& 0
\end{array}\]
Transformationen von anderen Standardformen in die kanonische Form:
\[\begin{array}{l@{\hspace{3mm}}rcl@{\hspace{8mm}}c@{\hspace{8mm}}rcl}
1)&\max  a^{T}y&&&&\max a^{T}x\\
&B y&=&d&\stackrel{x=y}{\longrightarrow}&\mbox{\scriptsize$\left(\begin{array}{r}B\\-B\\\end{array}
\right)$}x&\leqq&\mbox{\scriptsize$\left(\begin{array}{r}d\\-d\end{array}
\right)$}\\
&y&\geqq& 0&&x&\geqq& 0\vspace{5mm}\\
2)&\max a^{T}y&&&&\max \mbox{\scriptsize$\left(\begin{array}{r}a\\-a\end{array}\right)$}^{T}x\vspace{2mm}\\
&B y&\leqq&d&\stackrel{x=\mbox{\scriptsize$\left(\begin{array}{r}y+\\y-\end{array}
\right)$}}{\longrightarrow}&(B, -B)x&\leqq& d\\
&&&&&x&\geqq& 0\vspace{5mm}\\
3)&\max a^{T}y&&&&\max
\mbox{\scriptsize$\left(\begin{array}{r}a\\-a\end{array}\right)$}^{T}x\vspace{2mm}\\
&B y&=&d&\stackrel{x=\mbox{\scriptsize$\left(\begin{array}{r}y+\\y-\end{array}
\right)$}}{\longrightarrow}&   
\mbox{\scriptsize$\left(\begin{array}{rr}B&-B\\-B&B\end{array}\right)$}
x&\leqq& \mbox{\scriptsize$\left(\begin{array}{r}d\\-d\end{array}
\right)$}\\
&&&&&x&\geqq& 0
\end{array}\]
Als Hausaufgabe dürfen wir die Transformationen in die andere Richtung
machen :), (bei $B y=d$ geht es nicht).

Jedes LP kann also auch in die Form $\max \{c^{T}x| A x \leqq b\}$ transformiert
werden.

\paragraph{Farkas Lemma}
Wir suchen ein Analogon zur "`Aufwärmaufgabe"':\\
$A \in \RR^{m\times n}, \; \; b \in \RR^{m}$\\
$(\exists x) \; \;\; A x = b$ genau dann wenn $(\nexists y) \; \; y^{T}A
= 0, \; \; y^{T}b=-1$ für Ungleichungsprobleme der Form $A x\leqq b$


\paragraph{Fourier-Motzkin Elimination}

Durch Multiplikation der Ungleichungen mit positiven Skalaren und Umordnung
erhalten wir:
\[
(*)\left\{\begin{array}{rcl}
x_{1} + (a_{i}')^{T}  x'&\leqq& b^{*}_{i} \;\; \forall \; i\; \in
\{1,2,3,...m'\}\;\;\; \mbox{alle mit pos. Faktor vor $x_1$}\\
-x_{1} + (a_{i}')^{T}  x'&\leqq& b^{*}_{i} \;\; \forall \; i\; \in
\{m'+1,...m''\}\;\;\; \mbox{alle mit neg. Faktor vor $x_1$}\\
(a_{i}')^{T}  x'&\leqq& b^{*}_{i} \;\; \forall \; i\; \in
\{m''+1,...m\}\;\;\; \mbox{hier war gar kein $x_1$ drin}\\
\end{array}\right.
\]
\begin{tabular}{ll}
wobei&$x'=(x_{2}, x_{3},...x_{n})$\\
&$(a_{i}')^{T}$ ist die i-te Zeile von A ohne den ersten Eintrag
\end{tabular}

Die ersten beiden Zeilen von (*) sind äquivalent zu
\[\max_{m'+1\leqq j \leqq m''} \left((a_{j}')^{T} x' -b_{j}\right) \leqq x_{i}
\leqq \min_{1\leqq i \leqq m'} \left(b_{i} - (a_{i}')^{T}x'\right)\]

d.h. das Orginalsystem $A \leqq b$ hat eine Lösung genau dann wenn (in
Zukunft g.d.w.) das System:
\[
(**)\left\{\begin{array}{rcll}
(a_{j}')^{T}x' -b_{j} &\leqq& b_{i} - (a_{i}')^{T}x'& \forall \; i \; \in
\{1,2,3,...m'\}\\
%(a_{j}')^{T}x' -b_{j}&\leqq& b_{i} - (a_{i}')^{T}x'
&&&\forall \; i \; \in
\{m'+1,...m''\}\\
(a_{i}')^{T}x' &\leqq& b_{i}'& \forall \; i \; \in
\{m''+1,...m\}\\
\end{array}\right.
\]
eine Lösung hat ($x_{i}$ kann geeignet gewählt werden).\\
Umformulierung von (**)
\[
(***)\left\{\begin{array}{rcll}
(a_{i}'+a_{j}')^{T} x' &\leqq& b_{i} + b_{j}& \forall \; i \; \in
\{1,2,3,...m'\}\\
%(a_{i}'+a_{j}')^{T} x' &\leqq& b_{i} + b_{j}
&&& \forall \; i \; \in
\{m'+1,...m''\}\\
(a_{i}')^{T}x' &\leqq& b_{i}'& \forall \; i \; \in
\{m''+1,...m\}\\
\end{array}\right.
\]
Schreibe (***) als $A'x'\leqq b'$\\
Also $(\exists x) \; \; A x\leqq b$ g.d.w. $(\exists x') A' x' \leqq b'$

Fazit: Wir haben die Variable x eliminiert und haben jetzt in (***) $n-1$
Variablen und $m'(m''-m')+m-m''$ Ungleichungen $\rightarrow$ weniger
Variablen aber mehr Ungleichungen.

Diese Iteration liefert ein nicht polynomielles Verfahren zur Lösung von
Ungleichungssystemen.

\begin{lemma}
von Farkas für Ungleichungen\\
Seien $A \in \RR^{m\times n}$, $b\in \RR^{m}$\\
$(\exists x \in \RR^{n})$ $A x \leqq b\;$ g.d.w. $\;(\nexists y \in \RR^{m})$
$y\geqq 0$, $y^{T}A=0$, $y^{T}b < 0$\\
\end{lemma}

\begin{longtable}[l]{lp{13cm}}
\multicolumn{2}{l}{{\bf Beweis}}\\
"`$\Rightarrow$"'&$A x \leqq b$ habe eine Lösung $\bar{x}$\\
&Annahme:  $(\exists y \geqq 0) \; \; y^{T} A = 0, \; \; y^{T}b < 0$\\
&\[\begin{array}{rcl}
\Rightarrow 0 &>& y^{T}b\\
&\geqq & y^{T}A\bar{x}\\
&=& 0\; \; \lightning
\end{array}\]\\
&Mit $0>0$ ergibt sich also ein Widerspruch. Also gilt die Hin-Richtung.\\
"`$\Leftarrow$"'& $A x \leqq b$ habe keine Lösung.\\
& 1) Induktionsverankerung: $A \in \RR^{m\times 1}$
Nach Skalierung mit positiven Skalaren $\bar{y}$ haben alle Ungleichungen
die Form:\\
&\[(1) \; \; \; x_{n} \leqq \alpha ;\ \; \; \mbox{ oder } (2) \; \; \; 
-x_{n} \leqq \beta\]\\
&Nicht Lösbarkeit bedeutet also dass sich zwei Ungleichungen widersprechen.
O.b.d.A (=Ohne Beschränkung der Allgemeinheit) widersprechen sich die i-te 
Form (1) und die (j)-te der Form (2). D.h. $-\beta \leqq x_{n} \leqq \alpha$
mit $\alpha < -\beta$ d.h. $\alpha + \beta = \gamma < 0$\\

&Man wähle nun $\begin{array}[t]{rcl}
y_{i} &=& - \frac{\bar{y}_{i}}{\gamma} > 0 \;\;\; \mbox{ da } \gamma <
0\vspace{2mm}\\
y_{j} &=&- \frac{\bar{y}_{j}}{\gamma} > 0 \vspace{2mm}\\
y_{k} &=& 0 \; \; \; \mbox{ für } k \not\in \{i,j\}
\end{array}$\\
 
&\[\begin{array}{lrcl}
\Rightarrow& y &\geqq& 0\\
& y^{T}A &=& \left(\ldots,0, - \frac{\bar{y}_{i}}{\gamma},0,\ldots,0,
- \frac{\bar{y}_{j}}{\gamma},0,\ldots\right) \left(\begin{array}{c}
\vdots\\\ast\\\frac{1}{y_{i}}\\\ast\\\vdots\\\ast\\-\frac{1}{y_{j}}\\\ast
\\\vdots \end{array}\right)\\
&&=& - \frac{1}{\gamma} + \frac{1}{\gamma} = 0\\
&y^{T}b &=& - \frac{\alpha}{\gamma} - \frac{\beta}{\gamma} = 
- \frac{\alpha + \beta}{\gamma} = -1 < 0
\end{array}\]\\

&2) Induktion: $A \in \RR^{m \times n} \; \; n > 1 $\\
& $A x \leqq b$ hat keine Lösung $\Leftrightarrow A'x \leqq b'$ hat keine Lösung\\
& 3) Induktionsannahme $\begin{array}[t]{rcl}(\exists y' \geqq 0) 
\; \; y'^{T}A' &=& 0\\ y'^{T}b'&=&-1\end{array}$\\
&d.h.  $(\exists y' \geqq 0) y'^{T} [A'b'] =
(\underbrace{0,\ldots,0}_{n-1},-1) $\\
&Jede Zeile der Matrix $[0,\;A',\;b']$ ist Summe von nicht-negativ skalierten
Zeilen von $[A,\; b]$\\
&$\Rightarrow \; (\underbrace{0,\ldots,0}_{n},-1)$ ist nicht-negative
Linearkombination der Zeilen von $[A, \, b]$\\
&d.h. $(\exists y \geqq 0) \;\; y^{T} A = 0\; \; \; \; y^{T}b = -1$
\end{longtable}

\begin{korollar}
Lemma von Farkas, 1894

Seien $A\in \RR^{m\times n}, \; \; b \in \RR^{m}$
\[\begin{array}{rcllrcl}
(\exists x \in \RR^{n}) \; \; x &\geqq& 0 & g.d.w. & (\nexists y \in
\RR^{m}) \; \; y^{T}A &\geqq& 0\\
A x&=&b&&y^{T}b &<& 0
\end{array}\]
\end{korollar}

{\bf Beweis:} Für $\bar{A} = \left(
\begin{array}{c}A\\-A\\-I\end{array}\right)$ (mit $I=$ Einheitsmatrix) und
$\bar{b} = \left( \begin{array}{c}b\\-b\\0\end{array} \right)$ gilt:\\
\[\begin{array}{rcrcl}
(\exists x \geqq 0) \; A x = b &\stackrel{\mbox{\scriptsize Konstruktion}}{\Leftrightarrow}&
(\exists \bar{x}) \; \; \bar{A} \bar{x} &\leqq&\bar{b}\\
&\stackrel{\mbox{\scriptsize Satz 0.1 (Lemma für Ungl.)}}{\Leftrightarrow}&
(\nexists \bar{y} \geqq 0) \; \; \bar{y}^{T} \bar{A} &=&0\\
&&\bar{y}^{T}b&\leqq&0
\end{array}\]
\[\begin{array}{lcrcl}
\left[\bar{y}=\left(\begin{array}{c}u\\v\\w\end{array}\right)\right]
&\Leftrightarrow& \nexists
\left(\begin{array}{c}u\\v\\w\end{array}\right)\geqq 0 \; \; \; \;\;
u^{T}A - v^{T} A - w^{T} &=& 0\\
&&u^{T}b - v^{T} b &<& 0\\\\
\left[ y = u -v\right]&\Leftrightarrow&(\nexists y) \; \; y^{T} A &\geqq&
0\\ &&y^{T}b &<& 0 \hspace{7mm} \mbox{q.e.d}
\end{array}\]


Farkas Lemma kann auch anders betrachtet werden: 
\[\begin{array}{lrclrcl}
\mbox{Entweder}&(\exists x) A x &=& b, & x &\geqq& 0\\
\mbox{oder}&(\exists y) y^{T}A &\geqq& 0, & y^{T}b &<& 0
\end{array}\]
aber nicht beides. Dies kann man im $R^{2}$ auch graphisch darstellen in
dem man die Matrix $A=(A_{1}, A_{2}$ als Zusammensetzung aus zwei 
Vektoren $A_{1}$ und
$A_{2}$ betrachten, die jeweils eine "`halbe"' Ebene beschreiben. $y^{T}A 
\geqq$ beschreibt dann den Schnitt aus diesen beiden "`halben"' Ebenen. 
$y^{T}b < 0$ beschreibt wiederum eine "`halbe"' Ebene. Wenn jetzt nun $A x=b$ 
gelten soll so muss $b$ sich als eine Linearkombination der Vektoren $A_{1}$ 
und $A_{2}$ darstellen lassen. Anschaulich bedeutet dass das $b$ in der 
Zeichnung "`zwischen"' $A_{1}$ und  $A_{2}$ liegen muss dann wiederum wird
sich die Flächen die durch $y^{T}A \geqq 0$ und durch $y^{T}b < 0$
beschrieben werden niemals schneiden. 

Leider habe ich an dieser Stelle nicht die Zeit die Zeichnungen zu malen.
Freiwillige vor.

\begin{korollar} \label{eineLoesung}
Das System $A x \leqq b$ habe wenigstens eine Lösung. Dann erfüllt jede
Lösung $x$ von $A x\leqq b$ die Ungleichung $c^{T}x \leqq \delta$ genau dann,
wenn ein $y\geqq 0$ existiert, so dass $y^{T}A=c^{T}$ und $y^{T}b \leqq
\delta$.

\end{korollar}

Beweis:
\begin{longtable}{lp{13cm}}
"`$\Leftarrow$"'& $(\exists y \geqq 0) y^{T}A = c^{T}, \; \; y^{T}b \leqq
\delta$\\
&für alle $x \in \RR^{n}$ gilt:\\
&\[\begin{array}{r@{\Rightarrow}rcl}
A x \leqq b & y^{T} A x&\leqq& y^{T}b\\
&c^{T}x &\leqq& y^{T}b\\
&c^{T}x &\leqq & \delta
\end{array}\]\\
"`$\Rightarrow$"' & Annahme \\
&\[\begin{array}{rcl}(\nexists y \geqq 0 )y^{T}A&=&
c^{T}\\
y^{T}b \leqq \delta
\end{array}\]\\
&$\Rightarrow (\nexists y \geqq 0, \; \; \lambda \geqq 0)\; \; \;
(y^{T}, \; \lambda) \left(\begin{array}{cc}A&b\\ 0&1\end{array} \right) =
(c^{T},\; \delta)$\\
&$\stackrel{\mbox{\scriptsize (Farkas)}}{\Rightarrow}\begin{array}[t]{l@{\vspace{7mm}}rcl}
\left(\exists \left(\begin{array}{c}z\\\mu\end{array}\right)\right)&
\left(\begin{array}{cc}A&b\\0&1\end{array}\right)
\left(\begin{array}{c}z\\\mu\end{array}\right) &\geqq&0\\
&(c^{T},\; \delta) \left(\begin{array}{c}z\\\mu\end{array}\right) &<&0
\end{array}$
\end{longtable}

Fall 1 $\mu =0$:
\[\Rightarrow \left\{ \begin{array}{rcl}
A z &\geqq & 0\\
c^{T}z&<&0\\
\end{array}\right.\]
Sei $x^{0}$ Lösung von $A x\leqq b$. Für $r$ hinreichend groß gilt dann:
\[\begin{array}{rcl}A(x^{0} -r z) &\leqq& b\\
c^{T}(x^{0}-r z)&>&\delta\end{array}\]
$\bar{X}=x^{0} -r z$ ist Lösung die $c^{T}\bar{x} \leqq \delta$ verletzt.

Fall 2 $\mu > 0$:\\
Es gilt $A z + b \mu \geqq 0$
\[\begin{array}{crcl}
\Rightarrow & A z &\geqq & -b \mu\\
\Rightarrow & A\left(-\frac{z}{\mu} \right)&>& \delta
\end{array}\]
aber $c^{T}z + \delta\mu < 0$
\[\begin{array}{crcl}
\Rightarrow & c^{T}z &<& - \delta \mu \\
\Rightarrow & c^{T}\left(-\frac{z}{\mu}\right) &>& \delta
\end{array}\]

Also liefert die Lösung $\bar{x}=-\frac{z}{\mu}$ den Widerspruch.

\section{Dualitätstheorem}

Für das lineare Optimierungsproblem:
\[(P) \; \; \; \{\max c^{T}x | A x \leqq b\}\]
bezeichnen wir:
\begin{itemize} 
\item Jede Lösung $\bar{x}$ von $A x \leqq b$ als zulässige Lösung
\item Jede zulässige Lösung $\bar{x}$ die $c^{T} x $ maximiert als
Optimallösung.
\end{itemize}
Wir definieren das duale lineare Optimierungsproblem zu (P)
als (D) \[\min \{y^{T}b| y \geqq 0 , \; \; y^{T}A = c^{T} \}\]

\begin{satz} \label{SchwQual} Schwaches Dualitätstheorem\\
Seien $A \in \RR^{m \times n}, \; \; b \in \RR^{m}, \;\; c \in \RR^{n}$ und
$\bar{x}$ zulässig  für (P) und y zulässig für (D). Dann gilt $c^{T}\bar{x}
\leqq \bar{y}^{T} b$
\end{satz}
Beweis: $c^ {T}\bar{x} = (y^{T}A) \bar{x} = y^{T}(A x) \leqq y^{T}b$ q.e.d.

\begin{satz}
Dualitätstheorem (von Neumann, 1947)\\
Seien $A \in \RR^{m \times n}, \; \;  b \in \RR^{m}, \;\; c \in \RR^{n}$.
Dann gilt:
\[\max \{c^{T}x | A x \leqq b\}=\min \{y^{T}b| y \geqq 0 , \; \; y^{T}A = 
c^{T} \}\]
\end{satz}
Beweis:
\begin{enumerate}
\item Schwache Dualität (Satz \ref{SchwQual}) impliziert:
\[\begin{array}{ccl}
(\ast)& \delta :=& \sup \{c^{T}x | A x \leqq b\} \leqq \inf \{y^{T}b | y
\geqq 0, \; \; y^{T}A = c^{T} \}\\
& \Rightarrow& (\forall x) (A x \leqq b \Rightarrow c^{T}x \leqq \delta)\\
&\stackrel{\mbox{Korollar \ref{eineLoesung}}}{\Rightarrow}& (\exists y) \;
y\geqq 0, \; \; y^{T}A = c^{T},y ^{T}b \leqq \delta
\end{array}
\]
$\Rightarrow$ Das Infimum wird angenommen.\\
$\Rightarrow$ Das Minimum existiert und ist $=\delta$

\item $\begin{array}[t]{lrcl} \mbox {Annahme:}
&(\nexists x) A \leqq b, \; \; c^{T}x &\geqq& \delta\\
\Rightarrow& (\nexists x) \left(\begin{array}{c}A\\-c^{T}\end{array}\right)
x &\leqq&
\left(\begin{array}{c}b\\-\delta\end{array}\right)\vspace{2mm}\\
\stackrel{\mbox{\scriptsize \begin{tabular}{c}Lemma von Farkas\\ für
Ungleichungen\end{tabular}}}{\Longrightarrow}&\left(\exists \vect{z\\\lambda} \in
\RR^{m+1}\right),\;\; \vect{z\\\lambda} &\geqq& 0 \; \; \mbox{bzw. }\;\; z
\geqq 0, \; \; \lambda \geqq 0\vspace{2mm}\\
&(z^{T},;\ \lambda) \vect{A\\-c^{T}} &=& 0 \; \;  \mbox{bzw. } \;\; z^{T} A -
\lambda c^{T} = 0\vspace{3mm}\\
&(z^{T}, \; \lambda )  \vect{b\\-\delta} &\leqq& 0 \; \;  \mbox{bzw. }\;\;  z^{T}b
- x \lambda \leqq 0
\end{array}$

$Ax \leqq b$ habe eine Lösung $x^{0}$\\
Annahme $ x = 0 \Rightarrow 0 = z^{T}A x^{0} \leqq z^{T}b < 0 \lightning$\\
Also gilt: $\lambda > 0$\\
Für $y=\frac{z}{\lambda}$ gilt $\begin{array}[t]{rcl} y &\geqq&0 \; \; \; 
\mbox{ da } \; \; \begin{array}{rcl}z&\geqq & 0\\\lambda &\geqq &
0\end{array}\\
y^{T} A &=& \frac{1}{\lambda} z^{T}A = \frac{1}{\lambda} \lambda c^{T} =
c^{T}\\
y^{T}b &=& \frac{1}{\lambda} z^{T}b < \frac{1}{\lambda} \lambda \delta =
\delta
\end{array}$\\
$\Rightarrow$ Infimum ist $< \delta \; \; \; \lightning$ zu ($\ast$)
\end{enumerate}

\section{Dualisieren von LPs}

Im folgenden Abschnitt beschrieb Prof. Jünger ein "`Kochrezept"' zum
Dualisieren von LPs. Gegeben sei das folgende primale Problem:
\[\begin{array}{rcl@{\hspace{7mm}}l}
\mbox{(P)}\hspace{10mm} \max a^{T}x + b^{T}y\\
A x + B y &=& c& [u]\\
C x + D y &\leqq& d \hspace{7mm}&[v]\\
x &\geqq& 0
\end{array}
\]

Wir erhalten folgendes duale Problem:

\[\begin{array}{rcl@{\hspace{7mm}}l}
\mbox{(D)}\hspace{10mm} \min u^{T}c + v^{T} d\\
u^{T}A+v^{T} C &\mbox{(Un)gleichungsbeziehung}& a^{T}&[x]\\
u^{T}B + v^{T}D &\mbox{(Un)gleichungsbeziehung}& b^{T}&[y]\\
\end{array}\]

Nun gilt: Multiplikatoren für Gleichungen ergeben nicht
vorzeichenbeschränkte Variablen und Multiplikatoren für Ungleichungen
ergeben Vorzeichenbeschränkte Variablen, in unserem Fall also:

\[\begin{array}{rcl@{\hspace{7mm}}l}
\mbox{(D)}\hspace{10mm} \min u^{T}c + v^{T} d\\
u^{T}A+v^{T} C &\geqq& a^{T}&[x]\\
u^{T}B + v^{T}D &=& b^{T}&[y]\\
v&\geqq& 0
\end{array}\]

Daraus kann man leicht die Spezialfälle ableiten:
\[\begin{array}{lrcl@{\hspace{3mm}}c@{\hspace{3mm}}lrcl}
(P)&\max a^{T}x&&&&(D)& \min v^{T}d\\
&C x &\leqq& d & \rightarrow&& v^{T}C &\geqq& a^{T}\\
&x &\geqq&0&&&v &\geqq& 0\\
(P)&\max a^{T}x&&&&(D)& \min v^{T}c\\
&A x &=& c & \rightarrow&& u^{T}A &\geqq& a^{T}\\
&x &\geqq&0\\
(P)&\max b^{T}y&&&&(D)& \min v^{T}d\\
&D y &\leqq& d & \rightarrow&& v^{T}D &=& b^{T}\\
&&&&&&v &\geqq& 0
\end{array}\]

\begin{satz} \label{KomplSchl}
Satz vom komplementären Schlupf.
Seien $A \in \RR^{m\times n},\;\; b \in \RR^{m},\; c \in \RR^{n}$\\
Sei $x^{\ast}$ zulässig für $\max \{c^{T}x | A x \leqq b\}$ und\\
sei $y^{\ast}$ zulässig für $\min \{y^{T}b|y\geqq 0, \;\; y^{T}A= c^{T}\}$\\
Dann sind $x^{\ast}$ und $y^{\ast}$ Optimallösungen wenn gilt:\\
$y_{i}^{\ast} = 0$ oder $a_{i} x^{\ast} = b_{i} \; \; \forall i$\\
wobei $a_{i}$ die i-te Zeile von $A$ bezeichnet. 
\end{satz}

{
\newcommand{\xs}{\mbox{$x^{\ast}$}}
\newcommand{\ys}{\mbox{$y^{\ast}$}}


Beweis:\\
\begin{longtable}{cp{13cm}}
"`$\Rightarrow$"'& $x^{\ast}$ und $y^{\ast}$ seien Optimallösungen\\
& sei $i \in \{1,2,\ldots,m\}$ mit $y_{i}^{\ast} > 0$ und $a_{i}x^{\ast} <
b$\\
&\[\begin{array}{crcl}
\Rightarrow&0&=& \ys^{T}b -c^{T}\xs\\
&&=&\ys^{T}b - \ys^{T}A\xs\\
&&=&\ys^{T}(b-A\xs)\\
&&\geqq& y_{i}^{\ast} (b_{i} -a_{i}\xs)\\
&&>&0 \;\;\;\; \lightning
\end{array}\]\\
"`$\Leftarrow$"'&Für alle $i\in \{1,2,\ldots,m\}$ gilt $y_{i}^{\ast} = 0$
oder $a_{i} \xs = b_{i}$\\
&\[\begin{array}{crcl}
\Rightarrow&\ys^{T}b - c^{T}\xs &=& \ys^{T} (b-A\xs)\\
&&=&\displaystyle \sum_{i=1}^{m} y_{i}^{\ast} (b_{i} - a_{i} \xs)\\\
&&=&0 \hspace{7mm} \mbox{q.e.d.}
\end{array}\]
Dann folgt wegen starker Dualität \xs und \ys sind Optimallösungen.
\end{longtable}
}

\section{Der Simplexalgorithmus}

Es folgt eine Skizze des Verfahrens für LPs in der Form:
\[\begin{array}{rcl}
(P) \hspace{10mm} \max c^{T} x\\
A x&=&b\\
x&\geqq& 0
\end{array}
\]

mit $A \in \RR^{m \times n}$, $b \in \RR^{m}$, $c\in\RR^{n}$,
rang$(A)=m$.\\
Sei T die Menge der Spaltenindizes von $A$. Für $B\subseteq T$ sei $A_{B}$ die
Submatrix von A mit den Spalten aus $B$ und $c_{B}$ der Subvektor von $c$
mit den entsprechenden Komponenten.

Falls $|B| = m$ und die Spalten von $A_{B}$ linear unabhängig sind so ist
$B$ eine Basis von $A$, b.z.w. $A_{B}$ eine Basismatrix. Für eine Basis B
ist die Lösung x von $A x=b$ mit $ x=\vect{x_{B}\\x_{N}}$:
\[\begin{array}{rcl}
N&=& T \wout B, \hspace{7mm} \mbox{($N =$ die die nicht in der 
Basis sind)}\\
x_{B} &=& A_{B}^{-1} b, \; \; \;\; x_{N} =0
\end{array}\]
die zu B gehörige Basislösung.\\
$x_{B}$ ist die eindeutige Lösung des Gleichungssystems $A_{B} x_{B} = b$.\\
Weiterhin ist $y_{T}=c_{B}^{T}A_{B}^{-1}$ ( die eindeutige Lösung von 
$y^{T}A_{B} = c_{B}^{T})$\\
eine Lösung des dualen LPs:
\[\begin{array}{rcl}
(D) \hspace{10mm} \min y^{T} b\\
y^{T}A&\geqq&c^{T}\\
\end{array}\]
Basislösungen $x$ und $y$ haben die Eigenschaft $x_{i} > 0 \Rightarrow
y^{T}a_{i} = c_{i}$, wobei $a_{i}$ die i-te Spalte von A ist.

D.h. $x$ und $y$ sind zulässig für $(P)$ und $(D)$.\\
$\stackrel{\mbox{\scriptsize Satz vom kompl. Schlupf \ref{KomplSchl}}}{\Rightarrow}$
$x$ und $y$ sind optimal.

Eine Basis $B$ ist zulässig falls $x_{B} \geqq 0$.\\
Der Simplexalgorithmus generiert eine Folge von zulässigen Basen. Jedes mal
wird $y$ auf duale Zulässigkeit überprüft.\\
$y$ dual zulässig $\rightarrow$ STOP (Optimalität)

Sei $x$ zulässige aber nicht optimale Basislösung.\\
Wähle $i\in T$ mit $y^{T}a_{i} < c_{i}$\\
Plan: In $B$ wird ein Index entfernt und $i$ hinzugefügt.

Wir lösen $A_{B}z = a_{i}$ (d.h. z ist die i-te Spalte nach
Transformation durch die Basis).\\
Für $\epsilon > 0$ ersetzen wir $x_{B}$ durch $x_{B} - \epsilon z$ und
setzen $x_{i} = \epsilon$ und erhalten eine neue Lösung von $A x=b$ mit
Zielfunktionswert:\\
\[\begin{array}{rcl}
c_{B}^{T} (x_{B} - \epsilon z) + c_{i} \epsilon &=& c_{B}^{T} x_{B} +
\epsilon (c_{i} -c_{B}^{T} z)\\
&=&c_{B}^{T} x_{B} + \epsilon (c_{i} -y^{T}A_{B} z)\\
&=&c_{B}^{T} x_{B} + \epsilon (c_{i} -y^{T} a_{i})\\
\end{array}\] 
Es gilt $c_{i} - y^{T} a_{i} > 0$, d.h. jedes $\epsilon > 0$ verbessert die
Lösung.\\
Die neue Lösung soll zulässig bleiben, d.h. wir wählen:
\[ \max \{\epsilon | x_{B} -\epsilon {z} \geqq 0 \}\]
Falls das Maximum nicht existiert so ist (P) unbeschränkt $\rightarrow$
STOP (Unbeschränktheit)\\
sonst existiert ein Index $J$ mit $z > 0$ und $(x_{B} - \epsilon_{z})_{j}
=0$. Wir wählen $j$ als die Basis verlassenden Index. Dieser Test heißt
"`ratio test"' (Verhältnistest).\\
$j\in B$ mit $z_{j} > 0$ und Verhältnis $\frac{x_{j}}{z_{j}}$ minimal unter
$\frac{x_{k}}{z_{k}}$ mit $k \in B$\\
Die neue Basis ist also $(B \wout \{j\}) \cup \{i\}$.

\subsection{Simplex Algorithmus-Zusammenfassung}

An dieser stelle eine Formulierung in Pseudo-Code
%{ 
%\renewcommand{\labelitemi}{\mbox{}}
%\renewcommand{\labelitemii}{\mbox{}}
%\renewcommand{\labelitemiii}{\mbox{}}
%\renewcommand{\labelitemiv}{\mbox{}}

\begin{algorithmic}
\LOOP
\STATE Finde die eindeutige Lösung von $y^{T}A_{B} = c_{B}^{T}$
\IF{für alle $i \in T \wout B$ gilt: $y^{T} a_{i} \geqq c_{i}$}
\STATE STOP: die Lösung ist optimal.
\ELSE 
\STATE wähle $i$ mit $y^{T}a_{i} < c_{i}$
\STATE Finde die eindeutige Lösung von $A_{B}z=a_{i}$
\STATE Finde größtes $\epsilon \geqq 0$ mit $ x_{b} - \epsilon z \geqq 0$
\IF{ $\nexists \epsilon$ }
\STATE STOP: Problem ist unbeschränkt
\ELSE
\STATE wähle $j \in B$ mit $z_{j} > 0$ und $(x_{B} - \epsilon_{z})_{j}
= 0$
\STATE Ersetze $B$ durch $(B \cup \{i\}) \wout \{j\}$
\STATE Ersetze $x_{B}$ durch $x_{B} - \epsilon z$
\STATE  Setze $x_{i} = \epsilon$ 
\ENDIF
\ENDIF
\ENDLOOP
\end{algorithmic}

\subsection{Beispielrechnung mit dem Simplexalgorithmus}

Gegeben ist das folgende lineare Problem:
\[
\begin{array}{rrcrcrcl}
\max&3x_{1}&+&5x_{2}&+&4x_{3}\\
\mbox{s.t.}&2x_{1}&+&2x_{2}&+&x_{3} &\leqq& 3\\
&x_{1}&+&2x_{2}&+&4x_{3}&\leqq&5\\
&\multicolumn{5}{r}{x_{1},\; x_{2}, \; x_{3}}&\geqq&0
\end{array}
\]
Einführung von Schlupfvariablen um Gleichungen zu erhalten:
\[
\begin{array}{rrcrcrcrcrcl}
\max&3x_{1}&+&5x_{2}&+&4x_{3}\\
\mbox{s.t.}&2x_{1}&+&2x_{2}&+&x_{3}&+&x_{4}&&&=& 3\\
&x_{1}&+&2x_{2}&+&4x_{3}&&&+&x_{5}&=&5\\
&\multicolumn{9}{r}{x_{1},\; x_{2}, \; x_{3},\; x_{4},\; x_{5}}&\geqq&0
\end{array}
\]
ergibt die Form:
\[\begin{array}{rcl}
\max c^{T}x\\
A x&=&b\\
x &\geqq& 0
\end{array}\]
mit $\begin{array}[t]{rcl}A &=& \mat{rrrrr}{2&2&1&1&0\\1&2&4&0&1} \hspace{4mm}
b=\vect{3\\5}\vspace{2mm}\\
c^{T}&=&\mat{rrrrr}{3&5&4&0&0}
\end{array}$

Die Anfangsbasis sei nun $B = \{4,\;5\}$ d.h. $A_{B} = \mat{rr}{1&0\\0&1}$
\[x_{B} = A_{B}^{-1}b=\vect{3\\5}=\vect{x_{4}\\x_{5}}\] 
{\bf 1. Iteration:}\\
\[y^{T}A_{B} = c_{b}^{T} \Rightarrow (y_{1}, \; y_{2}) \mat{rr}{1&0\\0&1} = 
(0, \; 0) \rightarrow y = \vect{0\\0}\]
Für jedes $i$ aus $\{1, \; 2,\; 3\}$ gilt:
\[ y^{T}a_{i} = 0 < c_{i}\]
Wir nehmen $i=2$
\[A_{B} z = a_{i} \; :\hspace{3mm} \mat{rr}{1&0\\0&1} \vect{z_{4}\\z_{5}} =
\vect{2\\2} \rightarrow z = \vect{2\\2}\]
größtes $\epsilon$ mit:
\[x_{b} - \epsilon z \geqq 0  \; :\hspace{3mm} \vect{3\\5} - \epsilon
\vect{2\\2} \geqq 0\]
\[\rightarrow \epsilon = \frac{3}{2}\]

Für $j=4$ ist $z_{j} =2 > 0$ und $\begin{array}[t]{cl}&(x_{B} -\epsilon z)\\ 
=&x_{4} -\epsilon z_{4}\\
=&3- \frac{3}{2} \cdot 2 = 0\end{array}$

Neue Basis: $B=\{2,\; 5\}$
\[A_{B}=\mat{rr}{2&0\\2&1} \hspace{5mm} x_{B} = \vect{x_{2}\\x_{5}} =
\vect{\frac{3}{2}\\2}\]

{\bf 2. Iteration}\\
\[y^{T} \mat{rr}{2&0\\2&1} = (5, \; 0) \rightarrow y =
\vect{\frac{5}{2}\\0}\]
Überprüfe $y^{T}a_{i} < c_{i}$
\[\begin{array}{l@{:\hspace{4mm}}rcrcl}
\mbox{für } x_{1}& \mat{rr}{\frac{5}{2}&0} \vect{2\\1} &=& 5&\geqq&3
\vspace{3mm}\\
\mbox{für }x_{2}& \mat{rr}{\frac{5}{2}&0} \vect{1\\0}&=&\frac{5}{2}
&\geqq&0\vspace{3mm}\\
\mbox{für }x_{3}& \mat{rr}{\frac{5}{2}&0} \vect{1\\4}&=&\frac{5}{2} &<& 4
\rightarrow i=3\vspace{3mm}\\
A_{B}z = a_{i} & \mat{rr}{2&0\\2&1} \vect{z_{2}\\z_{5}} &=&
\vect{1\\4} &\rightarrow& z=\vect{\frac{1}{2}\\3}\end{array}\]
Größtes $\epsilon$ mit:
\[x_{B} -\epsilon z \geqq 0: \;\; \vect{\frac{3}{2}\\2} - \epsilon
\vect{\frac{1}{2}\\3} \geqq 0\]
\[\rightarrow \epsilon = \frac{2}{3} \rightarrow j = 5\]

neue Basis $B = \{2, \; 3\}$
\[A_{B} = \mat{rr}{2&1\\2&4} \hspace{4mm} x_{B} = \vect{x_{2}\\x_{3}} =
\vect{\frac{7}{6}\\\frac{2}{3}}\]

{\bf 3. Iteration:}
\[y^{T} \mat{rr}{2&1\\2&4} = (5, \; 4) \rightarrow y =
\vect{2\\\frac{1}{2}}\]
überprüfe $y^{T}a_{i} < c_{i}$
\[\begin{array}{l@{:\hspace{4mm}}rcrcl}
\mbox{für } x_{1}& \mat{rr}{2&\frac{1}{2}} \vect{2\\1}&=&
\frac{9}{2}&\geqq&
3\vspace{3mm}\\
\mbox{für } x_{4}& \mat{rr}{2&\frac{1}{2}} \vect{1\\0} &=& 2&\geqq
&0\vspace{3mm} \\
\mbox{für } x_{5}& \mat{rr}{2&\frac{1}{2}} \vect{0\\1} &=& \frac{1}{2}&\geqq&
0\end{array}\]
$\rightarrow$ Lösung ist optimal.\\
{\bf Frage:} Terminiert das Verfahren?\\
Wenn immer $\epsilon > 0:$ Ja denn es gibt nur endlich viele Basen.\\
{\bf Problem:} Im Fall $\epsilon = 0$ haben wir keine ZF-Verbesserung. Eine
Folge solcher degenerierter Iterationen kann zurück zur selben Basis
führen. $\rightarrow$ keine Terminierung.

Aber es gibt Regeln zur Wahl von $i$ und $j$ die solche "`Zyklen"'
verhindern. Mit einer solchen sog. Pivotregel terminiert das Verfahren
immer. Blands Regel: Nimm den kleinsten Index.

Wie erhält man nun aber die erste zulässige Basis:
\begin{itemize}
\item  Man multipliziere die
Gleichungen mit negativer rechter Seite mit $-1$, so dass das neue System
die rechte Seite $b\geqq0$ hat.
\item  Man löse das folgende LP:
\[\begin{array}{rcl}
(\mbox{HLP}) \hspace{6mm} \max \displaystyle \sum^{n}_{i=1} -x_{i}'\vspace{2mm}\\
(A, \; I) \vect{x\\x'} &=&b\vspace{2mm}\\
\vect{x\\x'}&\geqq&0
\end{array}\]
mit $I$ als erste Basismatrix ($I$ natürlich immer linear unabhängig).

{\bf Anmerkung:} Es gilt (P) hat eine zulässige Lösung g.d.w. (HLP) eine
Optimallösung mit Wert 0 hat.

\item Ist der Optimalwert von (HLP) kleiner als 0:
\begin{itemize}
\item[\mbox{}] STOP: (P) hat keine Zulässige Lösung
\end{itemize}
sonst sei $B$ die optimale Basis.
\item Löse:
\[\begin{array}{rcl}
(\mbox{P}') \hspace{6mm} \max (c^{T}, \; 0^{T}) \vect{x\\x'}\vspace {2mm}\\
s.t. \hspace{6mm} (A, I) \vect{x\\x'} &=&b\vspace{2mm}\\
\vect{x\\x'} \geqq 0
\end{array}\]
mit Startbasis $B$.

{\bf Anmerkung:} Es gibt Techniken, um die künstliche Variablen im Laufe 
des Verfahrens zu minimieren und zwar die künstlichen Nichtbasisvariablen 
sofort und die künstlichen Basisvariablen durch geeignete Pivots.
\end{itemize}


